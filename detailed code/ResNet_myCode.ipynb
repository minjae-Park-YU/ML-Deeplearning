{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet_myCode.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOLpyks+KBXf+1pu1tKRap0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"aoSFjsPLVvOZ"},"source":["import torchvision\r\n","import torchvision.transforms as transforms\r\n","\r\n","transform_train = transforms.Compose([\r\n","    transforms.RandomCrop(32, padding=4),\r\n","    transforms.RandomHorizontalFlip(),\r\n","    transforms.ToTensor(),\r\n","])\r\n","\r\n","transform_test = transforms.Compose([\r\n","    transforms.ToTensor(),\r\n","])\r\n","\r\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\r\n","train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [40000, 10000])\r\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\r\n","\r\n","partition = {'train': train_dataset, 'val':val_dataset, 'test':test_dataset}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cR5MOkgQVso1"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.backends.cudnn as cudnn\r\n","import torch.optim as optim\r\n","import os\r\n","import numpy as np\r\n","import argparse\r\n","from copy import deepcopy\r\n","import time\r\n","\r\n","\r\n","# ResNet18을 위해 최대한 간단히 수정한 BasicBlock 클래스 정의\r\n","class BasicBlock(nn.Module):\r\n","    def __init__(self, in_channels, out_dim, stride=1):\r\n","        super(BasicBlock, self).__init__()\r\n","\r\n","        # 3x3 필터를 사용 (너비와 높이를 줄일 때는 stride 값 조절)\r\n","        self.conv1 = nn.Conv2d(in_channels, out_dim, kernel_size=3, stride=stride, padding=1, bias=False)\r\n","        self.bn1 = nn.BatchNorm2d(out_dim) # 배치 정규화(batch normalization)\r\n","\r\n","        # 3x3 필터를 사용 (패딩을 1만큼 주기 때문에 너비와 높이가 동일)\r\n","        self.conv2 = nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1, bias=False)\r\n","        self.bn2 = nn.BatchNorm2d(out_dim) # 배치 정규화(batch normalization)\r\n","\r\n","        self.shortcut = nn.Sequential() # identity인 경우 -> 굳이 이 부분을 sequential로 만들어야 하는가?\r\n","        if stride != 1: # stride가 1이 아니라면, Identity mapping이 아닌 경우\r\n","            self.shortcut = nn.Sequential(\r\n","                nn.Conv2d(in_channels, out_dim, kernel_size=1, stride=stride, bias=False),\r\n","                nn.BatchNorm2d(out_dim)\r\n","            )\r\n","\r\n","    def forward(self, x):\r\n","        out = F.relu(self.bn1(self.conv1(x)))\r\n","        out = self.bn2(self.conv2(out))\r\n","        out += self.shortcut(x) # (핵심) skip connection\r\n","        out = F.relu(out)\r\n","        return out\r\n","\r\n","\r\n","# ResNet 클래스 정의\r\n","class ResNet(nn.Module):\r\n","    def __init__(self, block, num_blocks, num_classes, in_channels, stride=1):\r\n","        super(ResNet, self).__init__()\r\n","        self.in_channel = 64\r\n","\r\n","        # 64개의 3x3 필터(filter)를 사용\r\n","        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\r\n","        self.bn1 = nn.BatchNorm2d(64)\r\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\r\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\r\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\r\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\r\n","        self.linear = nn.Linear(512, num_classes)\r\n","\r\n","    def _make_layer(self, block, out_dim, num_blocks, stride):\r\n","        strides = [stride] + [1] * (num_blocks - 1)  # 첫 번째 stride 외에 나머지 stride는 1로 고정 \r\n","        layers = []\r\n","        for stride in strides:\r\n","            layers.append(block(self.in_channel, out_dim, stride))\r\n","            self.in_channel = out_dim # 다음 레이어를 위해 채널 수 변경\r\n","        return nn.Sequential(*layers)\r\n","\r\n","    def forward(self, x):\r\n","        out = F.relu(self.bn1(self.conv1(x)))\r\n","        out = self.layer1(out)\r\n","        out = self.layer2(out)\r\n","        out = self.layer3(out)\r\n","        out = self.layer4(out)\r\n","        out = F.avg_pool2d(out, 4)\r\n","        out = out.view(out.size(0), -1)\r\n","        out = self.linear(out)\r\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cy5irOOEOTwk"},"source":["def train(net, partition, optimizer, criterion, args):\r\n","    train_loader = torch.utils.data.DataLoader(partition['train'], batch_size=args.train_batch_size, shuffle=True, num_workers=4)\r\n","    net = torch.nn.DataParallel(net) # colab은 다중 GPU를 지원하기 때문에 가능한듯\r\n","    cudnn.benchmark = True  # 자동 튜너를 활성화해서 입력 이미지가 변하지 않는 모델이면 초기 시간은 걸리지만, 최종 시간은 짧아짐\r\n","\r\n","    net.train()\r\n","    train_loss = 0\r\n","    correct = 0\r\n","    total = 0\r\n","\r\n","    for batch_idx, data in enumerate(train_loader, 0):\r\n","        optimizer.zero_grad()\r\n","        inputs, targets = data\r\n","        inputs = inputs.cuda()\r\n","        targets = targets.cuda()\r\n","        outputs = net(inputs)\r\n","\r\n","        loss = criterion(outputs, targets)\r\n","        loss.backward()\r\n","\r\n","        optimizer.step()\r\n","        train_loss += loss.item()\r\n","        _, predicted = outputs.max(1) # 1. row 값 중에 최댓값 (_ 처리해서 무시)\r\n","                                             # 2. 최댓값의 column index  (predicted로 받아옴)\r\n","\r\n","        total += targets.size(0)\r\n","        correct += predicted.eq(targets).sum().item()\r\n","        \r\n","    train_loss = train_loss / len(train_loader)\r\n","    train_acc = 100 * correct / total\r\n","    return net, train_loss, train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhjLISF7Ro9_"},"source":["def validate(net, partition, criterion, args):\r\n","    val_loader = torch.utils.data.DataLoader(partition['val'], \r\n","                                            batch_size=args.test_batch_size, \r\n","                                            shuffle=False, num_workers=4)\r\n","    net.eval()\r\n","\r\n","    correct = 0\r\n","    total = 0\r\n","    val_loss = 0 \r\n","    with torch.no_grad():\r\n","        for data in val_loader:\r\n","            inputs, targets = data\r\n","            inputs = inputs.cuda()\r\n","            targets = targets.cuda()\r\n","            outputs = net(inputs)\r\n","\r\n","            loss = criterion(outputs, targets)\r\n","            \r\n","            val_loss += loss.item()\r\n","            _, predicted = outputs.max(1)\r\n","            total += targets.size(0)\r\n","            correct += predicted.eq(targets).sum().item()\r\n","\r\n","        val_loss = val_loss / len(val_loader)\r\n","        val_acc = 100 * correct / total\r\n","    return val_loss, val_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJ8xuO9WVxE6"},"source":["def test(net, partition, args):\r\n","    test_loader = torch.utils.data.DataLoader(partition['test'], batch_size=100, shuffle=False, num_workers=4)\r\n","\r\n","    net.eval()\r\n","\r\n","    correct = 0\r\n","    total = 0\r\n","\r\n","    with torch.no_grad():\r\n","        for data in test_loader:\r\n","            inputs, targets = data\r\n","            inputs = inputs.cuda()\r\n","            targets = targets.cuda()\r\n","\r\n","            outputs = net(inputs)\r\n","\r\n","            _, predicted = outputs.max(1)\r\n","            correct += predicted.eq(targets).sum().item()\r\n","            total += targets.size(0) # batch_size\r\n","\r\n","        test_acc = 100 * correct / total\r\n","\r\n","    return test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NClpwTNUUAHg"},"source":["def experiment(partition, args):\r\n","\r\n","    net = ResNet(block = args.block,\r\n","                 num_blocks = args.num_blocks,\r\n","                 num_classes = args.num_classes,\r\n","                 in_channels = args.in_channels,\r\n","                 stride = args.stride\r\n","                 )\r\n","    \r\n","    net.cuda()\r\n","\r\n","    criterion = nn.CrossEntropyLoss()\r\n","    if args.optim == 'SGD':\r\n","        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\r\n","    elif args.optim == 'RMSprop':\r\n","        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\r\n","    elif args.optim == 'Adam':\r\n","        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\r\n","    else:\r\n","        raise ValueError('In-valid optimizer choice')\r\n","\r\n","    train_losses = []\r\n","    val_losses = []\r\n","    train_accs = []\r\n","    val_accs = []\r\n","        \r\n","    for epoch in range(args.epoch):  # loop over the dataset multiple times\r\n","        ts = time.time()\r\n","        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\r\n","        val_loss, val_acc = validate(net, partition, criterion, args)\r\n","        te = time.time()\r\n","        \r\n","        train_losses.append(train_loss)\r\n","        val_losses.append(val_loss)\r\n","        train_accs.append(train_acc)\r\n","        val_accs.append(val_acc)\r\n","        \r\n","        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\r\n","        \r\n","    test_acc = test(net, partition, args)    \r\n","    \r\n","    result = {}\r\n","    result['train_losses'] = train_losses\r\n","    result['val_losses'] = val_losses\r\n","    result['train_accs'] = train_accs\r\n","    result['val_accs'] = val_accs\r\n","    result['train_acc'] = train_acc\r\n","    result['val_acc'] = val_acc\r\n","    result['test_acc'] = test_acc\r\n","    return vars(args), result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9X35ornVNmgI"},"source":["import hashlib\r\n","import json\r\n","from os import listdir\r\n","from os.path import isfile, join\r\n","import pandas as pd\r\n","\r\n","def save_exp_result(setting, result):\r\n","    exp_name = setting['exp_name']\r\n","    del setting['epoch']\r\n","    del setting['test_batch_size']\r\n","\r\n","    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\r\n","    filename = './results/{}-{}.json'.format(exp_name, hash_key)\r\n","    result.update(setting)\r\n","    with open(filename, 'w') as f:\r\n","        json.dump(result, f)\r\n","\r\n","    \r\n","def load_exp_result(exp_name):\r\n","    dir_path = './results'\r\n","    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\r\n","    list_result = []\r\n","    for filename in filenames:\r\n","        if exp_name in filename:\r\n","            with open(join(dir_path, filename), 'r') as infile:\r\n","                results = json.load(infile)\r\n","                list_result.append(results)\r\n","    df = pd.DataFrame(list_result) # .drop(columns=[])\r\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8TstJ8PARufC"},"source":["def adjust_learning_rate(optimizer, epoch):\r\n","    lr = learning_rate\r\n","    if epoch >= 100:\r\n","        lr /= 10\r\n","    if epoch >= 150:\r\n","        lr /= 10\r\n","    for param_group in optimizer.param_groups:\r\n","        param_group['lr'] = lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40rN8RTiVG7L"},"source":["# ====== Random Seed Initialization ====== #\r\n","seed = 123\r\n","np.random.seed(seed)\r\n","torch.manual_seed(seed)\r\n","\r\n","parser = argparse.ArgumentParser()\r\n","args = parser.parse_args(\"\")\r\n","args.exp_name = \"exp1_lr_model_code\"\r\n","\r\n","# ====== Model ====== #\r\n","args.in_channels = 3\r\n","args.block = BasicBlock\r\n","args.num_blocks = [2, 2, 2, 2]\r\n","args.num_classes = 10\r\n","args.stride = 1\r\n","\r\n","# ====== Regularization ======= #\r\n","args.l2 = 0.00001\r\n","\r\n","# ====== Optimizer & Training ====== #\r\n","# args.optim = 'SGD' #'RMSprop' #SGD, RMSprop, ADAM...\r\n","args.lr = 0.0015\r\n","args.epoch = 10\r\n","\r\n","args.train_batch_size = 512\r\n","args.test_batch_size = 1024\r\n","\r\n","# ====== Experiment Variable ====== #\r\n","name_var1 = 'lr'\r\n","name_var2 = 'optim'\r\n","list_var1 = [0.0001, 0.00001]\r\n","list_var2 = ['RMSprop', 'Adam']\r\n","\r\n","\r\n","for var1 in list_var1:\r\n","    for var2 in list_var2:\r\n","        setattr(args, name_var1, var1)\r\n","        setattr(args, name_var2, var2)\r\n","        print(args)\r\n","                \r\n","        setting, result = experiment(partition, deepcopy(args))\r\n","        #save_exp_result(setting, result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiE6cuYPXMeY"},"source":[""],"execution_count":null,"outputs":[]}]}